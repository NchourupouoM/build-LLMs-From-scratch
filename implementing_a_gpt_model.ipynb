{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the configuration of the small GPT-2 model via the following\n",
    "Python dictionary, which we will use in the code examples later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\":1024,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\":False \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"vocab_size\" refers to a vocabulary of 50,257 words, as used by the\n",
    "BPE tokenizer from chapter 2.\n",
    "- \"context_length\" denotes the maximum number of input tokens the\n",
    "model can handle, via the positional embeddings discussed in chapter 2.\n",
    "- \"emb_dim\" represents the embedding size, transforming each token into\n",
    "a 768-dimensional vector.\n",
    "- \"n_heads\" indicates the count of attention heads in the multi-head\n",
    "attention mechanism, as implemented in chapter 3.\n",
    "- \"n_layers\" specifies the number of transformer blocks in the model,\n",
    "which will be elaborated on in upcoming sections.\n",
    "- \"drop_rate\" indicates the intensity of the dropout mechanism (0.1\n",
    "implies a 10% drop of hidden units) to prevent overfitting, as covered in\n",
    "chapter 3.\n",
    "- \"qkv_bias\" determines whether to include a bias vector in the Linear\n",
    "layers of the multi-head attention for query, key, and value\n",
    "computations. We will initially disable this, following the norms of\n",
    "modern LLMs, but will revisit it in chapter 6 when we load pretrained\n",
    "GPT-2 weights from OpenAI into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder GPT model architecture class\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the steps shown in Figure 4.4 P(118), we tokenize a batch consisting\n",
    "of two text inputs for the GPT model using the tiktoken tokenizer introduced\n",
    "in chapter 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting token IDs for the two texts are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize a new 124 million parameter DummyGPTModel instance and\n",
    "feed it the tokenized batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor has two rows corresponding to the two text samples. Each\n",
    "text sample consists of 4 tokens; each token is a 50,257-dimensional vector,\n",
    "which matches the size of the tokenizer's vocabulary.\n",
    "\n",
    "\n",
    "The embedding has 50,257 dimensions because each of these dimensions\n",
    "refers to a unique token in the vocabulary. At the end of this chapter, when\n",
    "we implement the postprocessing code, we will convert these 50,257-\n",
    "dimensional vectors back into token IDs, which we can then decode into\n",
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate the example shown in Figure 4.5 via the following code,\n",
    "where we implement a neural network layer with 5 inputs and 6 outputs that\n",
    "we apply to two input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) \n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row in the mean tensor above contains the mean value for the first\n",
    "input row, and the second output row contains the mean for the second input\n",
    "row.\n",
    "\n",
    "\n",
    "Using keepdim=True in operations like mean or variance calculation ensures\n",
    "that the output tensor retains the same shape as the input tensor, even though\n",
    "the operation reduces the tensor along the dimension specified via dim. For\n",
    "instance, without keepdim=True, the returned mean tensor would be a 2-\n",
    "dimensional vector [0.1324, 0.2170] instead of a 2Ã—1-dimensional matrix\n",
    "[[0.1324], [0.2170]]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration of the dim parameter when calculating the mean of a tensor. For\n",
    "instance, if we have a 2D tensor (matrix) with dimensions [rows, columns], using dim=0 will\n",
    "perform the operation across rows (vertically, as shown at the bottom), resulting in an output\n",
    "that aggregates the data for each column. Using dim=1 or dim=-1 will perform the operation\n",
    "across columns (horizontally, as shown at the top), resulting in an output aggregating the data for\n",
    "each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Next, let us apply layer normalization to the layer outputs we obtained earlier. The operation consists of subtracting the mean and dividing by the square root of the variance (also known as standard deviation):\n",
    "\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value 9.9341e-09 in the output tensor is the scientific notation\n",
    "for 9.9341e Ã— 10-9, which is 0.0000000099341 in decimal form. This value is very\n",
    "close to 0, but it is not exactly 0 due to small numerical errors that can\n",
    "accumulate because of the finite precision with which computers represent\n",
    "numbers.\n",
    "\n",
    "\n",
    "To improve readability, we can also turn off the scientific notation when\n",
    "printing tensor values by setting sci_mode to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, in this section, we have coded and applied layer normalization in a\n",
    "step-by-step process. \n",
    "\n",
    "Let's now encapsulate this process in a PyTorch module\n",
    "that we can use in the GPT model later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer normalization class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim = True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) /torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific implementation of layer Normalization operates on the last\n",
    "dimension of the input tensor x, which represents the embedding dimension\n",
    "(emb_dim). The variable eps is a small constant (epsilon) added to thevariance to prevent division by zero during normalization. The scale and\n",
    "shift are two trainable parameters (of the same dimension as the input) that\n",
    "the LLM automatically adjusts during training if it is determined that doing\n",
    "so would improve the model's performance on its training task. This allows\n",
    "the model to learn appropriate scaling and shifting that best suit the data it is\n",
    "processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's now try the LayerNorm module in practice and apply it to the batch input \n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a feed forward network withGELU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, the ReLU activation function has been commonly used in deep\n",
    "learning due to its simplicity and effectiveness across various neural network\n",
    "architectures. However, in LLMs, several other activation functions are\n",
    "employed beyond the traditional ReLU. Two notable examples are GELU\n",
    "(Gaussian Error Linear Unit) and SwiGLU (Sigmoid-Weighted Linear Unit).\n",
    "\n",
    "\n",
    "GELU and SwiGLU are more complex and smooth activation functions\n",
    "incorporating Gaussian and sigmoid-gated linear units, respectively. They\n",
    "offer improved performance for deep learning models, unlike the simpler\n",
    "ReLU.\n",
    "\n",
    "\n",
    "The GELU activation function can be implemented in several ways; the exact\n",
    "version is defined as GELU(x)=x Î¦(x), where Î¦(x) is the cumulative\n",
    "distribution function of the standard Gaussian distribution. In practice,\n",
    "however, it's common to implement a computationally cheaper\n",
    "approximation (the original GPT-2 model was also trained with this\n",
    "approximation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An implementation of the GELU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to get an idea of what this GELU function looks like and how it\n",
    "compares to the ReLU function, let's plot these functions side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "# import numpy as np\n",
    "# gelu, relu = GELU(), torch.nn.ReLU()  # Assuming nn.ReLU() is the intended ReLU activation\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# # Some sample data\n",
    "# x = torch.linspace(-3, 3, 100)\n",
    "# y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "# plt.figure(figsize=(8, 3))\n",
    "# for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "#     plt.subplot(1, 2, i)\n",
    "#     plt.plot(x, y)\n",
    "#     plt.title(f\"{label} activation function\")\n",
    "#     plt.xlabel(\"x\")\n",
    "#     plt.ylabel(f\"{label}(x)\")\n",
    "#     plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the GELU function to implement the small neural network\n",
    "module, FeedForward, that we will be using in the LLM's transformer block\n",
    "later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A feed forward neural network module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]), \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the preceding code, the FeedForward module is a small\n",
    "neural network consisting of two Linear layers and a GELU activation\n",
    "function. In the 124 million parameter GPT model, it receives the input\n",
    "batches with tokens that have an embedding size of 768 each via the\n",
    "GPT_CONFIG_124M dictionary where GPT_CONFIG_124M[\"emb_dim\"] = 768."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's initialize a new FeedForward\n",
    "module with a token embedding size of 768 and feed it a batch input with 2\n",
    "samples and 3 tokens each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the shape of the output tensor is the same as that of the input\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a shortcut connection creates an alternative,\n",
    "shorter path for the gradient to flow through the network by skipping one or\n",
    "more layers, which is achieved by adding the output of one layer to the output\n",
    "of a later layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why these connections are also known as skipconnections. They play a crucial role in preserving the flow of gradients\n",
    "during the backward pass in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A neural network to illustrate shortcut connections\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            #implement 5 layers\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1])),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2])),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3])),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4])),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            #check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape==layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code implements a deep neural network with 5 layers, each consisting of\n",
    "a Linear layer and a GELU activation function. In the forward pass, we\n",
    "iteratively pass the input through the layers and optionally add the shortcut\n",
    "connections depicted in Figure 4.12 if the self.use_shortcut attribute is set\n",
    "to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this code to first initialize a neural network without shortcut\n",
    "connections. Here, each layer will be initialized such that it accepts an\n",
    "example with 3 input values and returns 3 output values. The last layer\n",
    "returns a single output value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for re\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a function that computes the gradients in the the model's\n",
    "backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0015313407639041543\n",
      "layers.1.0.weight has gradient mean of 0.0008734685834497213\n",
      "layers.2.0.weight has gradient mean of 0.002111609559506178\n",
      "layers.3.0.weight has gradient mean of 0.0030934568494558334\n",
      "layers.4.0.weight has gradient mean of 0.007880656979978085\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.2261723130941391\n",
      "layers.1.0.weight has gradient mean of 0.5056601762771606\n",
      "layers.2.0.weight has gradient mean of 0.3035311698913574\n",
      "layers.3.0.weight has gradient mean of 0.454271525144577\n",
      "layers.4.0.weight has gradient mean of 0.9717205166816711\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see based on the output above, shortcut connections prevent the gradients from vanishing in the early layers (towards layer.0)\n",
    "We will use this concept of a shortcut connection next when we implement a transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
