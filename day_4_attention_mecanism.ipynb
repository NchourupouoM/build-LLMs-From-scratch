{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement a simplified self-attention mechanism to\n",
    "compute these weights and the resulting context vector one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following input sentence, which has already been embedded\n",
    "into 3-dimensional vectors as discussed in chapter 2. We choose a small\n",
    "embedding dimension for illustration purposes to ensure it fits on the page\n",
    "without line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\USER\\Documents\\M Martin\\code LLMs From Scratch\\.torch_2_0_1\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11320\\3843080648.py\", line 2, in <module>\n",
      "    inputs = torch.tensor([\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11320\\3843080648.py:2: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  inputs = torch.tensor([\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your     (X^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (X^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (X^3)\n",
    "    [0.22, 0.58, 0.33], # with     (X^4)\n",
    "    [0.77, 0.25, 0.10], # one      (X^5)\n",
    "    [0.05, 0.80, 0.55]  # step     (X^6)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of implementing self-attention is to compute the intermediate\n",
    "values ω, referred to as attention scores, as illustrated in Figure 3.8 (P72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.8 The overall goal of this section is to illustrate the computation of the context vector `z(2)`\n",
    "using the second input sequence, `x(2)` as a query. This figure shows the first intermediate step,\n",
    "computing the attention scores `ω` between the query `x(2)` and all other input elements as a dot\n",
    "product. (Note that the numbers in the figure are truncated to one digit after the decimal point to\n",
    "reduce visual clutter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dot products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# We determine these scores by computing the dot product of the query, x(2), with every other input token:\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dot product is essentially just a concise way of multiplying two vectors\n",
    "element-wise and then summing the products, which we can demonstrate as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0],query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dot product is essentially just a concise way of multiplying two vectors\n",
    "element-wise and then summing the products, which we can demonstrate as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, as shown in Figure 3.9, we normalize each of the attention\n",
    "scores that we computed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"After computing the attention scores ω21 to ω2T with respect to the input query x(2),\n",
    "the next step is to obtain the attention weights α21 to α2T by normalizing the attention scores\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal behind the normalization shown in Figure 3.9 is to obtain\n",
    "attention weights that sum up to 1. This normalization is a convention that is\n",
    "useful for interpretation and for maintaining training stability in an LLM.\n",
    "Here's a straightforward method for achieving this normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_scores_2_tmp)\n",
    "print(\"Sum: \", attn_scores_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output shows, the attention weights now sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's more common and advisable to use the softmax function for\n",
    "normalization. This approach is better at managing extreme values and offers\n",
    "more favorable gradient properties during training. Below is a basic\n",
    "implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum()\n",
    "\n",
    "attn_scores_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights: \", attn_scores_2_naive)\n",
    "print(\"Sum: \", attn_scores_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the softmax function ensures that the attention weights are\n",
    "always positive. This makes the output interpretable as probabilities or\n",
    "relative importance, where higher weights indicate greater importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this naive softmax implementation (softmax_naive) may encounter\n",
    "numerical instability problems, such as overflow and underflow, when\n",
    "dealing with large or small input values. Therefore, in practice, it's advisable\n",
    "to use the PyTorch implementation of softmax, which has been extensively\n",
    "optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2,dim=0)\n",
    "print(\"Attention weights: \", attn_scores_2_naive)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we computed the normalized attention weights, we are ready for the\n",
    "final step illustrated in Figure 3.10: calculating the context vector z(2) by\n",
    "multiplying the embedded input tokens, x(i), with the corresponding attention\n",
    "weights and then summing the resulting vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context vector z(2) depicted in Figure 3.10 is calculated as a weighted\n",
    "sum of all input vectors. This involves multiplying each input vector by its\n",
    "corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # print(x_i)\n",
    "    # print(attn_weights_2[i])\n",
    "    context_vec_2 += attn_weights_2[i]*x_i \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will generalize this procedure for computing context\n",
    "vectors to calculate all context vectors simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same three steps as before, as summarized in Figure 3.12,\n",
    "except that we make a few modifications in the code to compute all context\n",
    "vectors instead of only the second context vector, z(2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in step 1 as illustrated in Figure 3.12 P(77), we add an additional for-loop to\n",
    "compute the dot products for all pairs of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the preceding attention score tensor, we used for-loops in\n",
    "Python. However, for-loops are generally slow, and we can achieve the same\n",
    "results using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, as illustrated in Figure 3.12, we now normalize each row so that the\n",
    "values in each row sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to step 3, the final step shown in Figure 3.12, let's briefly\n",
    "verify that the rows indeed all sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum:  1.0\n",
      "All row sums :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum: \", row_2_sum)\n",
    "print(\"All row sums : \", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third and last step, we now use these attention weights to compute all\n",
    "context vectors via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights@inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double-check that the code is correct by comparing the 2nd row with\n",
    "the context vector z(2) that we computed previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous 2nd context vector:  tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"previous 2nd context vector: \", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the self-attention mechanism step by step by introducing\n",
    "the three trainable weight matrices Wq, Wk, and Wv. These three matrices are\n",
    "used to project the embedded input tokens, x(i), into query, key, and value\n",
    "vectors as illustrated in Figure 3.14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in section 3.3.1, we defined the second input element x(2) as the query\n",
    "when we computed the simplified attention weights to compute the context\n",
    "vector z(2). Later, in section 3.3.2, we generalized this to compute all context\n",
    "vectors z(1) ... z(T) for the six-word input sentence \"Your journey starts with\n",
    "one step.\"\n",
    "\n",
    "\n",
    "Similarly, we will start by computing only one context vector, z(2), for\n",
    "illustration purposes. In the next section, we will modify this code to\n",
    "calculate all context vectors.\n",
    "\n",
    "\n",
    "Let's begin by defining a few variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the three weight matrices Wq, Wk, and Wv that are shown\n",
    "in Figure 3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are setting requires_grad=False to reduce clutter in the\n",
    "outputs for illustration purposes, but if we were to use the weight matrices for\n",
    "model training, we would set requires_grad=True to update these matrices\n",
    "during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the query, key, and value vectors as shown earlier in\n",
    "Figure 3.14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2@w_query\n",
    "key_2 = x_2@w_key\n",
    "value_2 = x_2@w_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight parameters vs attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape:  torch.Size([6, 2])\n",
      "values.shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ w_key\n",
    "values = inputs @ w_value\n",
    "print(\"keys.shape: \", keys.shape)\n",
    "print(\"values.shape: \", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention score computation is a dot-product computation similar to what we\n",
    "have used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we\n",
    "are not directly computing the dot-product between the input elements but using the query and\n",
    "key obtained by transforming the inputs via the respective weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# First, let's compute the attention score ω22:\n",
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Again, we can generalize this computation to all attention scores via matrix\n",
    "# multiplication\n",
    "attn_scores_2 = query_2@keys.T # All attention scores for given\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as illustrated in Figure 3.16, we compute the attention weights by\n",
    "scaling the attention scores and using the softmax function we used earlier..\n",
    "The difference to earlier is that we now scale the attention scores by dividing\n",
    "them by the square root of the embedding dimension of the keys, (note that\n",
    "taking the square root is mathematically the same as exponentiating by 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final step is to compute the context vectors, as illustrated in Figure\n",
    "3.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to section 3.3, where we computed the context vector as a weighted\n",
    "sum over the input vectors, we now compute the context vector as a weighted\n",
    "sum over the value vectors. Here, the attention weights serve as a weighting\n",
    "factor that weighs the respective importance of each value vector. Similar to\n",
    "section 3.3, we can use matrix multiplication to obtain the output in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only computed a single context vector, z(2). In the next section, we\n",
    "will generalize the code to compute all context vectors in the input sequence,\n",
    "z(1) to z(T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why query, key, and value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms \"key,\" \"query,\" and \"value\" in the context of attention mechanisms\n",
    "are borrowed from the domain of information retrieval and databases, where\n",
    "similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "A \"query\" is analogous to a search query in a database. It represents the\n",
    "current item (e.g., a word or token in a sentence) the model focuses on or\n",
    "tries to understand. The query is used to probe the other parts of the input\n",
    "sequence to determine how much attention to pay to them.\n",
    "\n",
    "\n",
    "The \"key\" is like a database key used for indexing and searching. In the\n",
    "attention mechanism, each item in the input sequence (e.g., each word in a\n",
    "sentence) has an associated key. These keys are used to match with the query.\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a\n",
    "database. It represents the actual content or representation of the input items.\n",
    "\n",
    "Once the model determines which keys (and thus which parts of the input)\n",
    "are most relevant to the query (the current focus item), it retrieves the\n",
    "corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compact self-attention class\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x@self.w_key\n",
    "        queries = x @ self.w_query\n",
    "        values = x @ self.w_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "                attn_scores/keys.shape[-1]**0.5, dim=-1\n",
    "            ) \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module,\n",
    "which is a fundamental building block of PyTorch models, which provides\n",
    "necessary functionalities for model layer creation and management.\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key,\n",
    "and W_value) for queries, keys, and values, each transforming the input\n",
    "dimension d_in to an output dimension d_out.\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention\n",
    "scores (attn_scores) by multiplying queries and keys, normalizing these\n",
    "scores using softmax. Finally, we create a context vector by weighting the\n",
    "values with these normalized attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this class as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the SelfAttention_v1 implementation further by utilizing\n",
    "PyTorch's nn.Linear layers, which effectively perform matrix multiplication\n",
    "when the bias units are disabled. Additionally, a significant advantage of\n",
    "using nn.Linear instead of manually implementing\n",
    "nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model\n",
    "training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.2 A self-attention class using PyTorch's Linear layers\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "                attn_scores/keys.shape[-1]**0.5, dim=-1\n",
    "            ) \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.1\n",
    "\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight\n",
    "initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in\n",
    "SelfAttention_v1, which causes both mechanisms to produce different\n",
    "results. To check that both implementations, SelfAttention_v1 and\n",
    "SelfAttention_v2, are otherwise similar, we can transfer the weightmatrices from a SelfAttention_v2 object to a SelfAttention_v1, such that\n",
    "both objects then produce the same results.\n",
    "\n",
    "Your task is to correctly assign the weights from an instance of\n",
    "SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need\n",
    "to understand the relationship between the weights in both versions. (Hint:\n",
    "nn.Linear stores the weight matrix in a transposed form.) After the\n",
    "assignment, you should observe that both instances produce the same outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# class SelfAttention_v2(nn.Module):\n",
    "#     def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "#         super().__init__()\n",
    "#         self.d_out = d_out\n",
    "#         self.w_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "#         self.w_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "#         self.w_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         keys = self.w_key@x\n",
    "#         queries = self.w_query@x\n",
    "#         values = self.w_value@x\n",
    "\n",
    "#         attn_scores = queries @ keys.T\n",
    "#         attn_weights = torch.softmax(\n",
    "#                 attn_scores/keys.shape[-1]**0.5, dim=-1\n",
    "#             ) \n",
    "#         context_vec = attn_weights @ values\n",
    "#         return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_in = inputs.shape[1]\n",
    "# d_out = 2\n",
    "# torch.manual_seed(123)\n",
    "# sa_v1 = SelfAttention_v2(d_in, d_out)\n",
    "# print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we modify the standard self-attention mechanism to create a\n",
    "causal attention mechanism, which is essential for developing an LLM in the\n",
    "subsequent chapters.\n",
    "\n",
    "Causal attention, also known as masked attention, is a specialized form of\n",
    "self-attention. It restricts a model to only consider previous and current inputs\n",
    "in a sequence when processing any given token. This is in contrast to the\n",
    "standard self-attention mechanism, which allows access to the entire input\n",
    "sequence at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to obtain the masked attention weight matrix in causal attention is to apply\n",
    "the softmax function to the attention scores, zeroing out the elements above the diagonal and\n",
    "normalizing the resulting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# In the first step illustrated in Figure 3.20, we compute the attention weights\n",
    "# using the softmax function as we have done in previous sections:\n",
    "\n",
    "queries = sa_v2.w_query(inputs)\n",
    "keys = sa_v2.w_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement step 2 in Figure 3.20 using PyTorch's tril function to\n",
    "create a mask where the values above the diagonal are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting mask is as follows:\n",
    "mask_simle = torch.tensor([[1., 0., 0., 0., 0., 0.],\n",
    "[1., 1., 0., 0., 0., 0.],\n",
    "[1., 1., 1., 0., 0., 0.],\n",
    "[1., 1., 1., 1., 0., 0.],\n",
    "[1., 1., 1., 1., 1., 0.],\n",
    "[1., 1., 1., 1., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_simple = attn_weights*mask_simle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the elements above the diagonal are successfully zeroed out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step in Figure 3.20 is to renormalize the attention weights to sum up\n",
    "to 1 again in each row. We can achieve this by dividing each element in each\n",
    "row by the sum in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple/row_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an attention weight matrix where the attention weights above the\n",
    "diagonal are zeroed out and where the rows sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply a mask and then renormalize the attention weights, it might\n",
    "initially appear that information from future tokens (which we intend to\n",
    "mask) could still influence the current token because their values are part of\n",
    "the softmax calculation. However, the key insight is that when we\n",
    "renormalize the attention weights after masking, what we're essentially doing\n",
    "is recalculating the softmax over a smaller subset (since masked positions\n",
    "don't contribute to the softmax value).\n",
    "\n",
    "In simpler terms, after masking and renormalization, the distribution of\n",
    "attention weights is as if it was calculated only among the unmasked\n",
    "positions to begin with. This ensures there's no information leakage from\n",
    "future (or otherwise masked) tokens as we intended\n",
    "\n",
    "While we could be technically done with implementing causal attention at\n",
    "this point, we can take advantage of a mathematical property of the softmax\n",
    "function and implement the computation of the masked attention weights\n",
    "more efficiently in fewer steps, as shown in Figure 3.21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function converts its inputs into a probability distribution. When\n",
    "negative infinity values (-∞) are present in a row, the softmax function treats\n",
    "them as zero probability. (Mathematically, this is because e-∞ approaches 0.)\n",
    "We can implement this more efficient masking \"trick\" by creating a mask\n",
    "with 1's above the diagonal and then replacing these 1's with negative infinity\n",
    "(-inf) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 6\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in the following mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is apply the softmax function to these masked results,\n",
    "and we are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now use the modified attention weights to compute the context\n",
    "vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = attn_weights @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout in deep learning is a technique where randomly selected hidden\n",
    "layer units are ignored during training, effectively \"dropping\" them out. This\n",
    "method helps prevent overfitting by ensuring that a model does not become\n",
    "overly reliant on any specific set of hidden layer units. It's important to\n",
    "emphasize that dropout is only used during training and is disabled afterward.\n",
    "\n",
    "\n",
    "In the transformer architecture, including models like GPT, dropout in the\n",
    "attention mechanism is typically applied in two specific areas: after\n",
    "calculating the attention scores or after applying the attention weights to thevalue vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will apply the dropout mask after computing the attention weights,\n",
    "as illustrated in Figure 3.22, because it's the more common variant in\n",
    "practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we apply PyTorch's dropout implementation first to a\n",
    "6×6 tensor consisting of ones for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half\n",
    "of the elements in the matrix are randomly set to zero. To compensate for the\n",
    "reduction in active elements, the values of the remaining elements in the\n",
    "matrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to\n",
    "maintain the overall balance of the attention weights, ensuring that the\n",
    "average influence of the attention mechanism remains consistent during both\n",
    "the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's apply dropout to the attention weight matrix itself:\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's apply dropout to the attention weight matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "batch = torch.stack((inputs, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape \n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the CausalAttention class as follows, similar to SelfAttention\n",
    "previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0,False)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical terms, implementing multi-head attention involves creating\n",
    "multiple instances of the self-attention mechanism (depicted earlier in Figure\n",
    "3.18 in section 3.4.1), each with its own weights, and then combining their\n",
    "outputs. Using multiple instances of the self-attention mechanism can be\n",
    "computationally intensive, but it's crucial for the kind of complex pattern\n",
    "recognition that models like transformer-based LLMs are known for.\n",
    "\n",
    "Figure 3.24 illustrates the structure of a multi-head attention module, which\n",
    "consists of multiple single-head attention modules, as previously depicted in\n",
    "Figure 3.18, stacked on top of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention module in this figure depicts two single-head attention\n",
    "modules stacked on top of each other. So, instead of using a single matrix Wv for computing the\n",
    "value matrices, in a multi-head attention module with two heads, we now have two value weight\n",
    "matrices: W\n",
    "v1 and Wv2. The same applies to the other weight matrices, Wq and Wk. We obtain\n",
    "two sets of context vectors Z1 and Z2 that we can combine into a single context vector matrix Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the main idea behind multi-head attention is to run the\n",
    "attention mechanism multiple times (in parallel) with different, learned linear\n",
    "projections -- the results of multiplying the input data (like the query, key,\n",
    "and value vectors in attention mechanisms) by a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of ourpreviously implemented CausalAttention module\n",
    "\n",
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_lenght, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in=d_in, d_out=d_out, context_length=context_lenght, dropout=dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
      "\n",
      "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]]], grad_fn=<CatBackward0>)\n",
      "context_vec.shape :  torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)\n",
    "print(\"context_vec.shape : \", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension of the resulting context_vecs tensor is 2 since we have\n",
    "two input texts (the input texts are duplicated, which is why the context\n",
    "vectors are exactly the same for those). The second dimension refers to the 6\n",
    "tokens in each input. The third dimension refers to the 4-dimensional\n",
    "embedding of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.2 : Returning 2-dimensional embedding vectors.\n",
    "\n",
    "Change the input arguments for the MultiHeadAttentionWrapper(...,\n",
    "num_heads=2) call such that the output context vectors are 2-dimensional\n",
    "instead of 4-dimensional while keeping the setting num_heads=2. Hint: You\n",
    "don't have to modify the class implementation; you just have to change one of\n",
    "the other input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]],\n",
      "\n",
      "        [[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]]], grad_fn=<CatBackward0>)\n",
      "context_vec.shape :  torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)\n",
    "print(\"context_vec.shape : \", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we created a MultiHeadAttentionWrapper to\n",
    "implement multi-head attention by stacking multiple single-head attention\n",
    "modules. This was done by instantiating and combining several\n",
    "CausalAttention objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An efficient multi-head attention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "    context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # assert d_out % num_heads == 0 #d_out must be divisible by num_headself.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2) \n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 1\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = mha(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
