{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of librairies \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of caracter:  20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data/the-verdict.txt\", 'r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of caracter: \", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# printing the first 100 caracters \n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some simple example text, we can use the re.split command with the\n",
    "following syntax to split a text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello, world. This, is a test\"\n",
    "result = re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the simple tokenization scheme above mostly works for separating\n",
    "the example text into individual words, however, some words are still\n",
    "connected to punctuation characters that we want to have as separate list\n",
    "entries. We also refrain from making all text lowercase because capitalization\n",
    "helps LLMs distinguish between proper nouns and common nouns,understand sentence structure, and learn to generate text with proper\n",
    "capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the regular expression splits on whitespaces (\\s) and commas,\n",
    "and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small remaining issue is that the list still includes whitespace characters.\n",
    "Optionally, we can remove these redundant characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle other types of pontuation (,.:?_!\"()'--)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'word', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Hello, word. Is this-- a test?\"\n",
    "result = re.split(r'([,.:?_!\"()\\']|--|\\s)',text_2)\n",
    "# removing white space \n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's\n",
    "entire short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above print statement outputs 4649, which is the number of tokens in this\n",
    "text (without whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and\n",
    "assigned it to a Python variable called preprocessed. Let's now create a list\n",
    "of all unique tokens and sort them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "\n",
    "Cette ligne de code Python effectue les opérations suivantes :\n",
    "\n",
    "set(preprocessed):\n",
    "\n",
    "preprocessed est probablement une liste ou un autre itérable contenant des mots ou des jetons (tokens).\n",
    "set(...) crée un ensemble (set) à partir de preprocessed. Un ensemble est une collection non ordonnée d'éléments uniques. En convertissant preprocessed en un ensemble, on supprime tous les mots en double.\n",
    "list(...):\n",
    "\n",
    "Cette opération reconvertit l'ensemble en une liste. Les ensembles n'ont pas d'ordre défini, tandis que les listes maintiennent l'ordre des éléments.\n",
    "sorted(...):\n",
    "\n",
    "Cette fonction trie la liste de mots par ordre alphabétique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining that the vocabulary size is 1,159 via the above code, we\n",
    "create the vocabulary and print its first 50 entries for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "# Creating a vocabulary\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual\n",
    "tokens associated with unique integer labels. Our next goal is to apply this\n",
    "vocabulary to convert new text into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python with an encode method\n",
    "that splits text into tokens and carries out the string-to-integer mapping to\n",
    "produce token IDs via the vocabulary. In addition, we implement a decode\n",
    "method that carries out the reverse integer-to-string mapping to convert the\n",
    "token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a simple text tokenizer \n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encoder(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decoder(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class\n",
    "and tokenize a passage from Edith Wharton's short story to try it out in\n",
    "practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiation of our tokenizer with the vocabulary in parameter \n",
    "tokenizer = SimpleTokenizerV1(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the test text \n",
    "text_3 = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride. \"The last but one,\" she corrected herself--\"but the other doesn't count, because he destroyed it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7, 1, 96, 615, 246, 745, 5, 1, 901, 298, 551, 6, 1, 246, 1013, 751, 363, 2, 995, 301, 5, 211, 541, 337, 596, 7]\n"
     ]
    }
   ],
   "source": [
    "# computing the token IDs \n",
    "ids = tokenizer.encoder(text_3)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see if we can turn these token IDs back into text using the decode\n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\" The last but one,\" she corrected herself --\" but the other doesn' t count, because he destroyed it.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, we can see that the decode method successfully\n",
    "converted the token IDs back into the original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. We implemented a tokenizer capable of tokenizing and detokenizing text based on a snippet from the training set. Let's now apply it to\n",
    "a new text sample that is not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_4 = \"Hello, do you like tea?\"\n",
    "# tokenizer.encoder(text=text_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the word \"Hello\" was not used in the The Verdict short\n",
    "story. Hence, it is not contained in the vocabulary. This highlights the need to\n",
    "consider large and diverse training sets to extend the vocabulary when\n",
    "working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will test the tokenizer further on text that contains\n",
    "unknown words, and we will also discuss additional special tokens that can\n",
    "be used to provide further context for an LLM during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons modifier le tokenizer pour utiliser un jeton <|unk|>\n",
    "s'il rencontre un mot qui ne fait pas partie du vocabulaire. De plus, nous ajoutons\n",
    "un jeton entre des textes sans rapport. Par exemple, lors de la formation de LLM de type GPT\n",
    "sur plusieurs documents ou livres indépendants, il est courant d'insérer un jeton\n",
    "avant chaque document ou livre qui suit une source de texte précédente, comme\n",
    "illustré à la figure 2.10. Cela aide le LLM à comprendre que, même si ces\n",
    "les sources de texte sont concaténées pour la formation, elles ne sont en fait pas liées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now modify the vocabulary to include these two special tokens, <unk>\n",
    "and <|endoftext|>, by adding these to the list of all unique words that we\n",
    "created in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of the print statement above, the new vocabulary size is\n",
    "1161 (the vocabulary size in the previous section was 1159)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional quick check, let's print the last 5 entries of the updated\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code output above, we can confirm that the two new special\n",
    "tokens were indeed successfully incorporated into the vocabulary. Next, we\n",
    "adjust the tokenizer from code listing 2.3 accordingly, as shown in listing 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple text tokenizer that handles unknown words\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encoder(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decoder(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try this new tokenizer out in practice. For this, we will use a simple\n",
    "text sample that we concatenate from two independent and unrelated\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text_5 = \"Hello, do you like tea?\"\n",
    "text_6 = \"In the sunlit terraces of the palace.\"\n",
    "text_7 = \"<|endoftext|> \".join((text_5,text_6))\n",
    "\n",
    "print(text_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's tokenize the sample text using the SimpleTokenizerV2 on the\n",
    "vocab we previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_v2 = SimpleTokenizerV2(vocab=vocab)\n",
    "print(tokenizer_v2.encoder(text_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that the list of token IDs contains 1159 for the\n",
    "<|endoftext|> separator token as well as two 1160 tokens, which are used for\n",
    "unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's de-tokenize the text for a quick sanity check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_v2.decoder(tokenizer_v2.encoder(text=text_7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on comparing the de-tokenized text above with the original input text,\n",
    "we know that the training dataset, Edith Wharton's short story The Verdict,\n",
    "did not contain the words \"Hello\" and \"palace.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the tokenizer used for GPT models also doesn't use an <|unk|>\n",
    "token for out-of-vocabulary words. Instead, GPT models use a byte pair\n",
    "encoding tokenizer, which breaks down words into subword units, which we\n",
    "will discuss in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte pair encoding (Encodage par paire d'octets): Using tiktoken librairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a simple tokenization scheme in the previous sections for\n",
    "illustration purposes. This section covers a more sophisticated tokenization\n",
    "scheme based on a concept called byte pair encoding (BPE). The BPE\n",
    "tokenizer covered in this section was used to train LLMs such as GPT-2,\n",
    "GPT-3, and the original model used in ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since implementing BPE can be relatively complicated, we will use an\n",
    "existing Python open-source library called tiktoken\n",
    "(https://github.com/openai/tiktoken), which implements the BPE algorithm\n",
    "very efficiently based on source code in Rust. Similar to other Python\n",
    "libraries, we can install the tiktoken library via Python's pip installer from the\n",
    "terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "# version of tiktoken\n",
    "from importlib.metadata import version\n",
    "import tiktoken \n",
    "print(\"tiktoken version: \",version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented\n",
    "previously via an encode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n"
     ]
    }
   ],
   "source": [
    "integers = bpe_tokenizer.encode(text_7, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then convert the token IDs back into text using the decode method,\n",
    "similar to our SimpleTokenizerV2 earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "strings = bpe_tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make two noteworthy observations based on the token IDs and\n",
    "decoded text above. First, the <|endoftext|> token is assigned a relatively\n",
    "large token ID, namely, 50256. In fact, the BPE tokenizer, which was used to\n",
    "train models such as GPT-2, GPT-3, and the original model used in\n",
    "ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being\n",
    "assigned the largest token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxièmement, le tokenizer BPE ci-dessus code et décode les mots inconnus, tels que\n",
    "comme \"someunknownPlace\" correctement. Le tokenizer BPE peut gérer n'importe quel\n",
    "mot inconnu. Comment y parvenir sans utiliser de jetons <|unk|> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm underlying BPE breaks down words that aren't in itspredefined vocabulary into smaller subword units or even individual\n",
    "characters, enabling it to handle out-of-vocabulary words. So, thanks to the\n",
    "BPE algorithm, if the tokenizer encounters an unfamiliar word during\n",
    "tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters ( p46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "\n",
    "Try the BPE tokenizer from the tiktoken library on the unknown words\n",
    "\"Akwirw ier\" and print the individual token IDs. Then, call the decode\n",
    "function on each of the resulting integers in this list to reproduce the mapping\n",
    "shown in Figure 2.11. Lastly, call the decode method on the token IDs to\n",
    "check whether it can reconstruct the original input, \"Akwirw ier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Encoder ===============================\n",
      "[33901, 86, 343, 86, 220, 959]\n",
      "\n",
      "\n",
      "====================== Decoder ===============================\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text_exe = \"Akwirw ier\"\n",
    "\n",
    "exe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"====================== Encoder ===============================\")\n",
    "integer_exe = exe_tokenizer.encode(text=text_exe, allowed_special={\"<|endoftext|>\"})\n",
    "print(integer_exe)\n",
    "print(\"\\n\")\n",
    "print(\"====================== Decoder ===============================\")\n",
    "strings_exe = exe_tokenizer.decode(integer_exe)\n",
    "print(strings_exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section covered the tokenization steps and conversion from\n",
    "string tokens into integer token IDs in great detail. The next step before we\n",
    "can finally create the embeddings for the LLM is to generate the input-target\n",
    "pairs required for training an LLM.\n",
    "\n",
    "\n",
    "What do these input-target pairs look like? As we learned in chapter 1, LLMs\n",
    "are pretrained by predicting the next word in a text, as depicted in figure 2.12 (P46)\n",
    "\n",
    "In this section we implement a data loader that fetches the input-target pairs\n",
    "depicted in Figure 2.12 from the training dataset using a sliding window\n",
    "approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# To get started, we will first tokenize the whole The Verdict short story we\n",
    "# worked with earlier using the BPE tokenizer introduced in the previoussection\n",
    "\n",
    "with open(\"Data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = bpe_tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove the first 50 tokens from the dataset for demonstration\n",
    "purposes as it results in a slightly more interesting text passage in the next\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest and most intuitive ways to create the input-target pairs for\n",
    "the next-word prediction task is to create two variables, x and y, where x\n",
    "contains the input tokens and y contains the targets, which are the inputs\n",
    "shifted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which are the inputs shifted by\n",
    "one position, we can then create the next-word prediction tasks depicted\n",
    "earlier in figure 2.12, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] =======> 4920\n",
      "[290, 4920] =======> 2241\n",
      "[290, 4920, 2241] =======> 287\n",
      "[290, 4920, 2241, 287] =======> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context,\"=======>\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything left of the arrow (=====>) refers to the input an LLM would\n",
    "receive, and the token ID on the right side of the arrow represents the target\n",
    "token ID that the LLM is supposed to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes, let's repeat the previous code but convert the token\n",
    "IDs into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and =======>  established\n",
      " and established =======>  himself\n",
      " and established himself =======>  in\n",
      " and established himself in =======>  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(bpe_tokenizer.decode(context),\"=======>\",bpe_tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created the input-target pairs that we can turn into use for the\n",
    "LLM training in upcoming chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only one more task before we can turn the tokens into embeddings, as\n",
    "we mentioned at the beginning of this chapter: implementing an efficient data\n",
    "loader that iterates over the input dataset and returns the inputs and targets as\n",
    "PyTorch tensors, which can be thought of as multidimensional arrays\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor\n",
    "containing the text that the LLM sees and a target tensor that includes the\n",
    "targets for the LLM to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the efficient data loader implementation, we will use PyTorch's built-in\n",
    "Dataset and DataLoader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset for batched inputs ant target \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids)- max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will use the GPTDatasetV1 to load the inputs in batches\n",
    "via a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DataLoader to generate batches with input-with pairs \n",
    "def create_dataloader_V1(txt, batch_size=4, max_length=256, stride=128,shuffle=True,drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt=txt, tokenizer=tokenizer, max_length=max_length,stride=stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset= dataset, batch_size=batch_size, shuffle=shuffle,drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context\n",
    "size of 4 to develop an intuition of how the GPTDatasetV1 class from listing\n",
    "2.5 and the create_dataloader_v1 function from listing 2.6 work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_V1(\n",
    "    txt= raw_text, batch_size=1, max_length=4, stride=1, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "# Executing the preceding code prints the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first_batch variable contains two tensors: the first tensor stores the\n",
    "input token IDs, and the second tensor stores the target token IDs. Since the\n",
    "max_length is set to 4, each of the two tensors contains 4 token IDs. Note\n",
    "that an input size of 4 is relatively small and only chosen for illustration\n",
    "purposes. It is common to train LLMs with input sizes of at least 256.\n",
    "\n",
    "\n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the first with the second batch, we can see that the second\n",
    "batch's token IDs are shifted by one position compared to the first batch (for\n",
    "example, the second ID in the first batch's input is 367, which is the first ID\n",
    "of the second batch's input). The stride setting dictates the number of\n",
    "positions the inputs shift across batches, emulating a sliding window\n",
    "approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2\n",
    "\n",
    "To develop more intuition for how the data loader works, try to run it with\n",
    "different settings such as max_length=2 and stride=2 and max_length=8 and\n",
    "stride=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "targets: \n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_V1(\n",
    "    txt= raw_text, batch_size=8, max_length=4, stride=1, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)\n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"targets: \\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step for preparing the input text for LLM training is to convert the\n",
    "token IDs into embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate how the token ID to embedding vector conversion works with\n",
    "a hands-on example. Suppose we have the following four input tokens with\n",
    "IDs 2, 3, 5, and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity and illustration purposes, suppose we have a small\n",
    "vocabulary of only 6 words (instead of the 50,257 words in the BPE\n",
    "tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT3, the embedding size is 12,288 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the vocab_size and output_dim, we can instantiate an embedding\n",
    "layer in PyTorch, setting the random seed to 123 for reproducibility purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(\n",
    "    vocab_size,\n",
    "    output_dim\n",
    ")\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weight matrix of the embedding layer contains small,\n",
    "random values. These values are optimized during LLM training as part of\n",
    "the LLM optimization itself, as we will see in upcoming chapters. Moreover,\n",
    "we can see that the weight matrix has six rows and three columns. There is\n",
    "one row for each of the six possible tokens in the vocabulary. And there is\n",
    "one column for each of the three embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we instantiated the embedding layer, let's now apply it to a token ID to\n",
    "obtain the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the embedding vector for token ID 3 to the previous\n",
    "embedding matrix, we see that it is identical to the 4th row (Python starts\n",
    "with a zero index, so it's the row corresponding to index 3). In other words,the embedding layer is essentially a look-up operation that retrieves rows\n",
    "from the embedding layer's weight matrix via a token ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have seen how to convert a single token ID into a threedimensional embedding vector. Let's now apply that to all four input IDs we\n",
    "defined earlier (torch.tensor([2, 3, 5, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embedding layers versus matrix multiplication\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we focused on very small embedding sizes in this chapter for\n",
    "illustration purposes. We now consider more realistic and useful embedding\n",
    "sizes and encode the input tokens into a 256-dimensional vector\n",
    "representation. This is smaller than what the original GPT-3 model used (in\n",
    "GPT-3, the embedding size is 12,288 dimensions) but still reasonable for\n",
    "experimentation. Furthermore, we assume that the token IDs were created by\n",
    "the BPE tokenizer that we implemented earlier, which has a vocabulary size\n",
    "of 50,257:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 256)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the data loader from section 2.6, Data sampling with a\n",
    "sliding window, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: \n",
      " tensor([[24818,   417,    12, 12239],\n",
      "        [  314,  3114,   379,   262],\n",
      "        [ 2156,   286,  4116,    13],\n",
      "        [  866,   262,  2119,    11],\n",
      "        [ 3363,    11,   340,   373],\n",
      "        [  198,     1,    40,  2900],\n",
      "        [  465, 14475,    13,   198],\n",
      "        [ 3081,   286,  2045,  1190]])\n",
      "\n",
      " inputs chape \n",
      ":  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4 \n",
    "dataloader = create_dataloader_V1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length\n",
    ")\n",
    "iter_dataloader = iter(dataloader)\n",
    "inputs_2, targets_2 = next(iter_dataloader)\n",
    "print(\"Token ID: \\n\" , inputs_2)\n",
    "print(\"\\n inputs chape \\n: \" , inputs_2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data\n",
    "batch consists of 8 text samples with 4 tokens each.\n",
    "\n",
    "Let's now use the embedding layer to embed these token IDs into 256-\n",
    "dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0673, -0.3863, -0.5027,  ..., -1.0610,  0.5170,  0.4422],\n",
      "         [-1.0366,  0.8779,  0.4560,  ..., -0.6743,  0.9195, -1.5858],\n",
      "         [-0.4357,  0.5339,  0.2413,  ...,  0.5189, -1.9390,  0.8580],\n",
      "         [ 1.1013, -0.2652,  0.7247,  ...,  0.3632, -0.5016,  0.1404]],\n",
      "\n",
      "        [[ 0.5786, -1.8926, -1.7647,  ..., -0.1368,  0.3491,  2.1122],\n",
      "         [-1.1059, -0.3257,  0.1568,  ...,  0.1386,  1.0505, -0.1206],\n",
      "         [-1.3986,  0.7459, -0.0840,  ..., -1.9663,  0.8480, -0.8870],\n",
      "         [-0.3962,  0.5593,  3.4120,  ...,  0.6146, -0.3981,  0.7999]],\n",
      "\n",
      "        [[ 0.4188, -0.2066, -1.1127,  ..., -0.1625,  1.9322,  0.6564],\n",
      "         [-0.5594,  1.2612, -0.9617,  ..., -0.8864, -0.0426, -2.1107],\n",
      "         [ 0.3512,  0.0883, -0.3792,  ..., -0.0899, -0.0107,  0.8041],\n",
      "         [-1.1577, -1.9382,  0.9027,  ..., -0.0718, -0.8468, -1.0623]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1437, -0.9782,  1.5918,  ...,  0.3068, -1.1135, -0.7202],\n",
      "         [ 1.2277, -0.4297, -2.2121,  ..., -0.1640, -0.3348, -0.0221],\n",
      "         [ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
      "         [-1.1626, -1.2506, -0.6494,  ...,  1.2301, -1.5976, -0.2130]],\n",
      "\n",
      "        [[-0.6694,  0.9102, -2.0349,  ..., -0.2177, -0.5799,  0.7145],\n",
      "         [-0.4156, -1.4310, -0.6640,  ..., -2.2738,  0.0814,  0.5976],\n",
      "         [-1.1577, -1.9382,  0.9027,  ..., -0.0718, -0.8468, -1.0623],\n",
      "         [-0.1437, -0.9782,  1.5918,  ...,  0.3068, -1.1135, -0.7202]],\n",
      "\n",
      "        [[-0.5819,  0.1400,  0.2161,  ..., -0.2513, -0.8626,  1.0159],\n",
      "         [-0.5594,  1.2612, -0.9617,  ..., -0.8864, -0.0426, -2.1107],\n",
      "         [ 0.8460, -0.7887, -0.3839,  ...,  1.0037, -0.1200,  1.7293],\n",
      "         [-0.1531,  0.9564, -0.1842,  ...,  1.8271, -0.3989, -0.0712]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs_2)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell based on the 8x4x256-dimensional tensor output, each token\n",
    "ID is now embedded as a 256-dimensional vector.\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create\n",
    "another embedding layer that has the same dimension as the token_embeddings_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the positional embedding tensor consists of four 256-\n",
    "dimensional vectors. We can now add these directly to the token embeddings,\n",
    "where PyTorch will add the 4x256-dimensional pos_embeddings tensor to\n",
    "each 4x256-dimensional token embedding tensor in each of the 8 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_embeddings we created, are the embedded input examples that can now be processed by the main LLM\n",
    "modules, which we will begin implementing in chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
