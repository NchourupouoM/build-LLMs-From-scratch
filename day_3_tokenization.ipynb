{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of librairies \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of caracter:  20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data/the-verdict.txt\", 'r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of caracter: \", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# printing the first 100 caracters \n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some simple example text, we can use the re.split command with the\n",
    "following syntax to split a text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello, world. This, is a test\"\n",
    "result = re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the simple tokenization scheme above mostly works for separating\n",
    "the example text into individual words, however, some words are still\n",
    "connected to punctuation characters that we want to have as separate list\n",
    "entries. We also refrain from making all text lowercase because capitalization\n",
    "helps LLMs distinguish between proper nouns and common nouns,understand sentence structure, and learn to generate text with proper\n",
    "capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the regular expression splits on whitespaces (\\s) and commas,\n",
    "and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small remaining issue is that the list still includes whitespace characters.\n",
    "Optionally, we can remove these redundant characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle other types of pontuation (,.:?_!\"()'--)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'word', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Hello, word. Is this-- a test?\"\n",
    "result = re.split(r'([,.:?_!\"()\\']|--|\\s)',text_2)\n",
    "# removing white space \n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's\n",
    "entire short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above print statement outputs 4649, which is the number of tokens in this\n",
    "text (without whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and\n",
    "assigned it to a Python variable called preprocessed. Let's now create a list\n",
    "of all unique tokens and sort them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "\n",
    "Cette ligne de code Python effectue les opérations suivantes :\n",
    "\n",
    "set(preprocessed):\n",
    "\n",
    "preprocessed est probablement une liste ou un autre itérable contenant des mots ou des jetons (tokens).\n",
    "set(...) crée un ensemble (set) à partir de preprocessed. Un ensemble est une collection non ordonnée d'éléments uniques. En convertissant preprocessed en un ensemble, on supprime tous les mots en double.\n",
    "list(...):\n",
    "\n",
    "Cette opération reconvertit l'ensemble en une liste. Les ensembles n'ont pas d'ordre défini, tandis que les listes maintiennent l'ordre des éléments.\n",
    "sorted(...):\n",
    "\n",
    "Cette fonction trie la liste de mots par ordre alphabétique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining that the vocabulary size is 1,159 via the above code, we\n",
    "create the vocabulary and print its first 50 entries for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "# Creating a vocabulary\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual\n",
    "tokens associated with unique integer labels. Our next goal is to apply this\n",
    "vocabulary to convert new text into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python with an encode method\n",
    "that splits text into tokens and carries out the string-to-integer mapping to\n",
    "produce token IDs via the vocabulary. In addition, we implement a decode\n",
    "method that carries out the reverse integer-to-string mapping to convert the\n",
    "token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a simple text tokenizer \n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encoder(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decoder(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class\n",
    "and tokenize a passage from Edith Wharton's short story to try it out in\n",
    "practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiation of our tokenizer with the vocabulary in parameter \n",
    "tokenizer = SimpleTokenizerV1(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the test text \n",
    "text_3 = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride. \"The last but one,\" she corrected herself--\"but the other doesn't count, because he destroyed it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7, 1, 96, 615, 246, 745, 5, 1, 901, 298, 551, 6, 1, 246, 1013, 751, 363, 2, 995, 301, 5, 211, 541, 337, 596, 7]\n"
     ]
    }
   ],
   "source": [
    "# computing the token IDs \n",
    "ids = tokenizer.encoder(text_3)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see if we can turn these token IDs back into text using the decode\n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\" The last but one,\" she corrected herself --\" but the other doesn' t count, because he destroyed it.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, we can see that the decode method successfully\n",
    "converted the token IDs back into the original text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
