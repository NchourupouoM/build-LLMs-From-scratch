{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A logistic regression forward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0] , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute in an output layer\n",
    "z = x1*w1+b\n",
    "a = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9183], grad_fn=<SigmoidBackward0>),\n",
       " tensor([2.4200], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.binary_cross_entropy(a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating w1 and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_output:int):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            #1st hidden layers\n",
    "            torch.nn.Linear(num_inputs,30), \n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30,20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # Output layer \n",
    "            torch.nn.Linear(20, num_output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of new NeuralNetwork model\n",
    "model = NeuralNetwork(num_inputs=50, num_output=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Printing our model summary \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parametres are:  2213\n"
     ]
    }
   ],
   "source": [
    "# Checking for total number of trainable parameters of our model \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "print(\"Total number of trainable parametres are: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0140,  0.1141, -0.0555, -0.0537, -0.1367,  0.1325, -0.0204, -0.0267,\n",
      "        -0.0182, -0.0141, -0.1084,  0.1293,  0.0982, -0.0130, -0.1166,  0.1030,\n",
      "        -0.1035,  0.1308, -0.0041, -0.1371, -0.0775, -0.1317,  0.1077,  0.1308,\n",
      "        -0.0525, -0.1021,  0.0685,  0.1336, -0.1189,  0.0956, -0.0956,  0.1397,\n",
      "        -0.0626, -0.1314, -0.0395, -0.0528,  0.1146,  0.0751, -0.0991, -0.0744,\n",
      "        -0.0441,  0.0133,  0.1055, -0.0314, -0.0122, -0.1392, -0.0051,  0.0971,\n",
      "        -0.0836,  0.0288], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can access the corresponding weight parameter matrix(of a specifique layer) as follows\n",
    "print(model.layers[0].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0306, -0.1222,  0.0285,  0.0298, -0.0192,  0.0409,  0.0888, -0.0540,\n",
      "        -0.0216, -0.0953, -0.0094,  0.0729,  0.1143, -0.1376,  0.1097,  0.0583,\n",
      "        -0.0987, -0.0095, -0.0149,  0.0376, -0.1405, -0.0510, -0.0211,  0.1295,\n",
      "         0.0778, -0.0690,  0.0241,  0.0044, -0.0577,  0.0354],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3135, 0.9837, 0.5922, 0.3073, 0.3608, 0.7955, 0.0664, 0.6533, 0.0822,\n",
       "         0.0200, 0.5744, 0.6328, 0.9363, 0.0790, 0.0242, 0.6049, 0.3515, 0.0839,\n",
       "         0.2789, 0.1170, 0.4995, 0.0073, 0.6496, 0.7226, 0.5823, 0.5777, 0.4274,\n",
       "         0.5367, 0.2473, 0.7602, 0.3540, 0.6743, 0.0882, 0.0440, 0.4770, 0.4211,\n",
       "         0.7572, 0.4479, 0.7891, 0.2577, 0.6708, 0.5272, 0.9537, 0.1902, 0.6983,\n",
       "         0.8500, 0.5506, 0.2846, 0.3462, 0.5554]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random training data \n",
    "X = torch.rand((1,50))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1796, -0.2231, -0.0342]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model (Execute forward pass of a model)\n",
    "out = model(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1796, -0.2231, -0.0342]])\n"
     ]
    }
   ],
   "source": [
    "# This tells PyTorch that it doesn't need to keep track of the\n",
    "# gradients, which can result in significant savings in memory and\n",
    "# computation.\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4039, 0.2700, 0.3261]])\n"
     ]
    }
   ],
   "source": [
    "# Using softmax fonction as the activation function \n",
    "with torch.no_grad():\n",
    "    out =torch.softmax(model(X),dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
