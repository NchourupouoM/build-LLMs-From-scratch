{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A logistic regression forward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0] , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute in an output layer\n",
    "z = x1*w1+b\n",
    "a = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9183], grad_fn=<SigmoidBackward0>),\n",
       " tensor([2.4200], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.binary_cross_entropy(a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating w1 and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs:int, num_output:int):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            #1st hidden layers\n",
    "            torch.nn.Linear(num_inputs,30), \n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30,20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # Output layer \n",
    "            torch.nn.Linear(20, num_output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of new NeuralNetwork model\n",
    "model = NeuralNetwork(num_inputs=50, num_output=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Printing our model summary \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parametres are:  2213\n"
     ]
    }
   ],
   "source": [
    "# Checking for total number of trainable parameters of our model \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "print(\"Total number of trainable parametres are: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1205,  0.1037, -0.1028, -0.1124, -0.0894,  0.0640, -0.0523,  0.0529,\n",
      "        -0.1200, -0.0858, -0.0519, -0.0278, -0.1079,  0.0926, -0.0334,  0.0454,\n",
      "         0.1000,  0.0263,  0.0387,  0.1365, -0.0638,  0.0448, -0.0629,  0.1011,\n",
      "         0.1129, -0.1304,  0.1207,  0.0675,  0.0616,  0.0582,  0.1176, -0.0187,\n",
      "        -0.1196, -0.0406, -0.0996,  0.0093, -0.0264, -0.0759, -0.0129,  0.1340,\n",
      "        -0.0112,  0.0045, -0.0221,  0.0222,  0.1260,  0.0865,  0.0502,  0.0307,\n",
      "         0.0333,  0.0546], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can access the corresponding weight parameter matrix(of a specifique layer) as follows\n",
    "print(model.layers[0].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0509,  0.0545, -0.0393,  0.0924, -0.1412, -0.1232, -0.1063,  0.0081,\n",
      "        -0.1249,  0.0101, -0.0019, -0.1298,  0.1388, -0.0330,  0.1017,  0.1247,\n",
      "        -0.0554, -0.0417,  0.1388,  0.0159,  0.1215,  0.0385,  0.0769, -0.1224,\n",
      "        -0.0279,  0.0991, -0.0079, -0.1229,  0.0148,  0.0132],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3306, 0.5374, 0.2845, 0.8459, 0.2232, 0.2083, 0.8169, 0.1084, 0.3285,\n",
       "         0.7185, 0.3624, 0.3084, 0.8893, 0.4179, 0.9741, 0.3697, 0.2397, 0.8936,\n",
       "         0.1443, 0.1365, 0.7625, 0.1632, 0.6641, 0.1525, 0.9830, 0.5936, 0.9120,\n",
       "         0.0146, 0.6323, 0.4743, 0.7467, 0.3545, 0.9994, 0.9815, 0.7399, 0.2057,\n",
       "         0.8742, 0.0138, 0.7676, 0.7481, 0.7570, 0.6432, 0.9111, 0.2246, 0.8668,\n",
       "         0.6961, 0.5131, 0.6978, 0.4537, 0.9035]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random training data \n",
    "X = torch.rand((1,50))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1152,  0.1256,  0.1569]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model (Execute forward pass of a model)\n",
    "out = model(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1152,  0.1256,  0.1569]])\n"
     ]
    }
   ],
   "source": [
    "# This tells PyTorch that it doesn't need to keep track of the\n",
    "# gradients, which can result in significant savings in memory and\n",
    "# computation.\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2789, 0.3549, 0.3662]])\n"
     ]
    }
   ],
   "source": [
    "# Using softmax fonction as the activation function \n",
    "with torch.no_grad():\n",
    "    out =torch.softmax(model(X),dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small toy dataset\n",
    "\n",
    "# Train data \n",
    "X_train = torch.tensor([\n",
    "    [-1.2,3.1],\n",
    "    [-0.9,2.9],\n",
    "    [-0.5,2.6],\n",
    "    [2.3,-1.1],\n",
    "    [2.7,-1.5],\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0,0,0,1,1])\n",
    "\n",
    "# test data\n",
    "X_test = torch.tensor([\n",
    "    [-0.8,2.8],\n",
    "    [2.6,-1.6],\n",
    "    ])\n",
    "\n",
    "y_test = torch.tensor([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Custom ToyDataset class's purpose is to use it to instantiate a pytorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X , y):\n",
    "        self.feature = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.feature[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ToyDataset(X_train,y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating data loaders \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# train DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size= 2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Test DataLoader \n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "\n",
      "\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "\n",
      "\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over the train_loader\n",
    "for idx, (x,y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x , y)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is desired to prevent deep neural networks getting caught in repetitive update\n",
    "cycles during training. To prevent this, it's recommended to set drop_last=True, which\n",
    "will drop the last batch in each epoch, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size= 2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [ 2.7000, -1.5000]]) tensor([1, 1])\n",
      "\n",
      "\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([0, 0])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate once more over the train_loader\n",
    "for idx, (x,y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x , y)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
